**Case study**: 
Given an unlabelled data $x$, and two summaries $y \in \{y_0, y_1\}$, we train our reward models by the judgment. The reward models are trained to predict which summary is preferred by a human. 

For example, given a ISOSMILE structure:
``` 
{"CID": 5745, "Isomeric SMILES": "CC(=O)OCC(=O)[C@]1(CC[C@@H]2[C@@]1(CC(=O)[C@H]3[C@H]2CCC4=CC(=O)CC[C@]34C)C)O"}
```

$s_1$ generated by Model1: ```This Isomeric SMILES represents a molecule with an ester group attached to a bicyclic structure consisting of two fused rings: a cyclohexane ring in a chair conformation and a cyclopentane ring, with both containing additional carbonyl groups and a methyl side chain, as well as a hydroxyl group on the cyclohexane ring, all specified with stereochemistry.```

$s_2$ generated by Model2: ```The Isomeric SMILES describes a molecule with a complex two-ring backbone that has various arms made up of oxygen, carbon, and hydrogen, including parts that resemble vinegar's key ingredient, with specific 3D arrangements.```

Human judges the $s_1$ (identified the ring structure of this sequence, which is obviously better than $s_2$).

Here we denote the preferred summary is $s_i$, the RM loss is expressed as:
$$
\text{loss}(\theta) = -\mathbb{E}_{(x,y_0,s_1,i) \sim \mathcal{D}} \left[ \log \left( \sigma \left( r_{\theta}(x, s_i) - r_{\theta}(x, s_{1-i}) \right) \right) \right]
$$
where $r_{\theta}(x, s)$ is the scalar output of the reward model for the data $x$ and summary $s$, parameterized by $\theta$, and $\mathcal{D}$ is the dataset consisting of human judgments. At the end of training, we normalize the reward model outputs such that the reference summaries from our dataset achieve a mean score of 0.